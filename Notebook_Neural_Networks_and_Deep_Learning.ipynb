{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook --  Neural Networks and Deep Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plastermodelsean/Deep-Learning-with-Python/blob/master/Notebook_Neural_Networks_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kVmeo2xkCZSv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2.1 Logistic Regression as a Neural Network#\n",
        "\n",
        "\n",
        "## 2.12 Logistic Regression ##\n",
        "\n",
        " \n",
        " Given x, want: $$\\hat{y} = P(y=1|x) ,x\\in\\mathcal{\\mathbb{R}^{n_x}},0\\leq\\hat{y}\\leq{1} \\tag{1}$$\n",
        " Parameters:$$\\mathcal{w}\\in\\mathbb{R}^{n_x},b\\in\\mathbb{R}\\tag{2}$$\n",
        " Output:$$\\hat{y} = \\sigma(\\mathcal{w}^{T}x+b)\\tag{3}$$\n",
        " \n",
        "where$$\\sigma(z) = \\cfrac{1}{1+e^{-z}}\\tag{4}$$\n",
        " If z is large, then:$$\\sigma(z) \\approx 1,$$\n",
        "Conversely, If z is a large negative number, then:$$\\sigma(z) \\approx 0.$$\n",
        "\n",
        "## 2.13 Logistic Regression Cost Function ##\n",
        "\n",
        "**Objective**\n",
        "\n",
        "Given$\\lbrace(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})\\rbrace$, want $ \\hat{y}^{(i)} \\approx y^{(i)}$.\n",
        "\n",
        "**Loss(error) Function**\n",
        "\n",
        "We won't use a square error here because it will lead to an un-convex problem and thus we cannot reach the global optima\n",
        "\n",
        "$$\\mathcal{L} = - (y\\log{\\hat{y}} +(1-y)\\log{(1-\\hat{y})})\\tag{1}$$\n",
        "\n",
        "If y = 1: $$\\mathcal{L}(\\hat{y},y) = -\\log{\\hat{y}}\\tag{2}$$\n",
        "\n",
        "we want $\\log{\\hat{y}}$ large, so we want $\\hat{y}\\to1$ ;\n",
        "\n",
        "If y = 0: $$ \\mathcal{L}(\\hat{y},y) = -\\log{(1-\\hat{y})}\\tag{3}$$\n",
        "\n",
        "we want $\\log{(1-\\hat{y})}$ large, so , we want $\\hat{y}\\to0$.\n",
        "\n",
        "**Cost Function**\n",
        "$$\\mathcal{J}(\\mathcal{w},b) = \\cfrac{1}{m}\\sum_{i = 1}^{m}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})\\tag{4}$$\n",
        "$$= -\\cfrac{1}{m}\\sum_{i = 1}^{m}[y^{(i)}\\log{\\hat{y}^{(i)}} + (1-y^{(i)})\\log{(1-\\hat{y}^{(i)})}]\\tag{5}$$\n",
        "\n",
        "The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.\n",
        "\n",
        "## 2.14 Gradient Descent ##\n",
        "\n",
        "**Objective**\n",
        "\n",
        "We want to find $\\mathcal{w},b$ that minimize $\\mathcal{J}(\\mathcal{w},b)$.\n",
        "\n",
        "**Gradient Descent**\n",
        "\n",
        "Repeat {\n",
        "$$\\mathcal{w} := \\mathcal{w} - \\alpha\\cfrac{\\partial \\mathcal{J}(\\mathcal{w},b)}{\\partial\\mathcal{w}}\\tag{1}$$\n",
        "\n",
        "$$b := b - \\alpha\\cfrac{\\partial \\mathcal{J}(\\mathcal{w},b)}{\\partial b}\\tag{2}$$\n",
        "}\n",
        "\n",
        "## 2.19 Logistic Regression Gradient Descent ##\n",
        "\n",
        "**Objective**\n",
        "\n",
        "Say we have $z$ to be the input of the sigmoid function and $a$ to be the output of logistic regression: \n",
        "\n",
        "$forward:$\n",
        "\n",
        "$$z = \\mathcal{w}^T x + b\\tag{1}$$\n",
        "\n",
        "$$a = \\sigma(z)\\tag{2}$$\n",
        "\n",
        "$$\\mathcal{L}(a,y) = - [y\\log{a} +(1 - y)\\log{(1-a)}]\\tag{3}$$\n",
        "\n",
        "$backprop:$\n",
        "\n",
        "\n",
        "$$\\mathrm{d}a = \\cfrac{\\mathrm{d}\\mathcal{L}(a,y)}{\\mathrm{d}a}$$\n",
        "$$ = -\\cfrac{y}{a} + \\cfrac{1 - y}{1 - a}\\tag{4}$$\n",
        "\n",
        "\n",
        "$$\\cfrac{\\mathrm{d}a}{\\mathrm{d}z} = a(1-a)\\tag{5}$$\n",
        "\n",
        "$$\\mathrm{d}z = \\cfrac{\\mathrm{d}\\mathcal{L}(a,y)}{\\mathrm{d}z}$$\n",
        "\n",
        "\n",
        "$$= \\cfrac{\\mathrm{d}\\mathcal{L}(a,y)}{\\mathrm{d}a}\\cdot\\cfrac{\\mathrm{d}a}{\\mathrm{d}z}$$\n",
        "\n",
        "$$= a - y\\tag{6}$$\n",
        "\n",
        "In practical, we usually use $\\mathrm{d}z $ to represent $\\cfrac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}z}$, use $\\mathrm{d}a $ to represent $\\cfrac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}a}$ and so on.\n",
        "\n",
        "Thus, the parameters will be updated by:\n",
        "\n",
        "$$\\mathcal{w} := \\mathcal{w} - \\alpha\\cdot\\mathrm{d}\\mathcal{w}\\tag{7}$$\n",
        "\n",
        "$$b := b - \\alpha\\cdot\\mathrm{d}b\\tag{8}$$\n",
        "\n",
        "## 2.20 Gradient Descent on m Examples ##\n",
        "\n",
        "Concretely, our logistic regression model is initialized by:\n",
        "\n",
        "$\\begin{matrix} n_x\\\\ \\mathcal{J} = 0, \\overbrace{\\mathrm{d}\\mathcal{w}_1 = 0,\\mathrm{d}\\mathcal{w}_2 = 0,\\cdots,\\mathrm{d}\\mathcal{w}_{n_x} = 0 },\\mathrm{d}b = 0. \\end{matrix}$\n",
        "\n",
        "$for\\, i=1 :m$\n",
        "\n",
        "$$z^{(i)} = \\mathcal{w}^T x^{(i)} + b\\tag{1}$$\n",
        "\n",
        "$$a^{(i)} = \\sigma(z^{(i)} )\\tag{2}$$\n",
        "\n",
        "$$\\mathcal{J} \\mathrel{+}= - [y^{(i)}\\log{a^{(i)}} +(1 - y^{(i)})\\log{(1-a^{(i)})}]\\tag{3}$$\n",
        "\n",
        "$$\\mathrm{d}z^{(i)} = a^{(i)} - y^{(i)}\\tag{4}$$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w}_1 \\mathrel{+}= x_1^{(i)}\\mathrm{d}z^{(i)} $$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w}_2 \\mathrel{+}= x_2^{(i)}\\mathrm{d}z^{(i)} $$\n",
        "\n",
        "$$\\begin{matrix}\\vdots \\end{matrix}$$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w}_{n_x} \\mathrel{+}= x_{n_x}^{(i)}\\mathrm{d}z^{(i)} \\tag{5}$$\n",
        "\n",
        "$$\\mathrm{d}b \\mathrel{+}= \\mathrm{d}z\\tag{6}$$\n",
        "\n",
        "$end$\n",
        "\n",
        "$$\\mathcal{J}\\mathrel{\\div}= m,$$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w}_1 \\mathrel{\\div}= m, $$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w}_2 \\mathrel{\\div}= m, $$\n",
        "\n",
        "$$\\begin{matrix}\\vdots \\end{matrix}$$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w}_{n_x}\\mathrel{\\div}= m,$$\n",
        "$$\\mathrm{d}b\\mathrel{\\div}= m\\tag{7}$$\n",
        "\n",
        "$thusly,$\n",
        "\n",
        "$$\\cfrac{\\partial{\\mathcal{J}}}{\\partial\\mathcal{w_j}}=\\mathrm{d}\\mathcal{w}_j$$\n",
        "\n",
        "$$\\mathcal{w_j} := \\mathcal{w}_j - \\alpha\\cdot\\mathrm{d}\\mathcal{w_j}$$\n",
        "\n",
        "$$b := b - \\alpha\\cdot\\mathrm{d}b\\tag{8}$$\n",
        "\n",
        "N.B.\n",
        "\n",
        "In the for loop depicited above, there is only one dw variable (i.e. no i superscripts in the for loop), as the value of dw in the code is cumulative."
      ]
    },
    {
      "metadata": {
        "id": "OVZdnlJ489Dy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2.2 Python and Vectorization#\n",
        "\n",
        "##2.21 Vectorization##\n",
        "\n",
        "Here is a vectorization demo written in python"
      ]
    },
    {
      "metadata": {
        "id": "MuWwJ7629_A3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38b1bd50-4568-420d-b149-f52973bb2c1a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1,2,3,4])\n",
        "print(a)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UVrBBd5q-Ho7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3576ca25-16c1-4689-bdd2-6098d59b665c"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "a = np.random.rand(1000000)\n",
        "b = np.random.rand(1000000)\n",
        "\n",
        "tic = time.time()\n",
        "c = np.dot(a,b)\n",
        "toc = time.time()\n",
        "print(c)\n",
        "print(\"Vectorized version:\" + str(1000*(toc-tic)) + \"ms\")\n",
        "\n",
        "c = 0\n",
        "tic = time.time()\n",
        "for i in range(1000000):\n",
        "  c += a[i]*b[i]\n",
        "toc = time.time()\n",
        "\n",
        "print(c)\n",
        "print(\"For loop version:\" + str(1000*(toc-tic)) + \"ms\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250264.67451881\n",
            "Vectorized version:1.2106895446777344ms\n",
            "250264.6745188151\n",
            "For loop version:438.8151168823242ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4nFW8GlpAx-i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.22 More Vectorization Examples##\n",
        "\n",
        "**Neural network programming guideline**\n",
        "\n",
        "Whenever possible, avoid explicit for-loops.\n",
        "\n",
        "**Numpy built-in functions**\n",
        "\n",
        "numpy.dot()\n",
        "\n",
        "A**\n",
        "\n",
        "$\\cdots$\n",
        "\n",
        "**Logistic regression derivatives**\n",
        "\n",
        "With the vectorization approach we can get rid of one for-loop:\n",
        "\n",
        "$\\mathcal{J} = 0,\\mathrm{d}\\mathcal{w}= numpy.zeros((n_x,1)),\\mathrm{d}b = 0.$\n",
        "\n",
        "$for\\, i=1 :m$\n",
        "\n",
        "$$z^{(i)} = \\mathcal{w}^T x^{(i)} + b\\tag{1}$$\n",
        "\n",
        "$$a^{(i)} = \\sigma(z^{(i)} )\\tag{2}$$\n",
        "\n",
        "$$\\mathcal{J} \\mathrel{+}= - [y^{(i)}\\log{a^{(i)}} +(1 - y^{(i)})\\log{(1-a^{(i)})}]\\tag{3}$$\n",
        "\n",
        "$$\\mathrm{d}z^{(i)} = a^{(i)} - y^{(i)}\\tag{4}$$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w}\\mathrel{+}= x^{(i)}\\mathrm{d}z^{(i)} \\tag{5}$$\n",
        "\n",
        "$$\\mathrm{d}b \\mathrel{+}= \\mathrm{d}z\\tag{6}$$\n",
        "\n",
        "$end$\n",
        "\n",
        "$$\\mathcal{J}\\mathrel{\\div}= m,$$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w}\\mathrel{\\div}= m,$$\n",
        "$$\\mathrm{d}b\\mathrel{\\div}= m\\tag{7}$$\n",
        "\n",
        "$thusly,$\n",
        "\n",
        "$$\\cfrac{\\partial{\\mathcal{J}}}{\\partial\\mathcal{w_j}}=\\mathrm{d}\\mathcal{w}_j$$\n",
        "\n",
        "$$\\mathcal{w_j} := \\mathcal{w}_j - \\alpha\\cdot\\mathrm{d}\\mathcal{w_j}$$\n",
        "\n",
        "$$b := b - \\alpha\\cdot\\mathrm{d}b\\tag{8}$$\n"
      ]
    },
    {
      "metadata": {
        "id": "qlmY6SOGMTBr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.23 Vectorizing Logistic Regression##\n",
        "\n",
        "**Forward Propgation**\n",
        "\n",
        "Before, we have to calculate one by one:\n",
        "\n",
        "$$z^{(1)} = \\mathcal{w}^Tx^{(1)} + b, a^{(1)} = \\sigma(z^{(1)});\\\\z^{(2)} = \\mathcal{w}^Tx^{(2)} + b, a^{(2)} = \\sigma(z^{(2)});\\\\\\vdots\\\\z^{(m)} = \\mathcal{w}^Tx^{(m)} + b, a^{(m)} = \\sigma(z^{(m)}).\\tag{1}$$\n",
        "\n",
        "Now, we want to implement a vectorized approach to calculate those numbers all at the same time.Like before, our m input examples look like this:\n",
        "\n",
        "$$X=\\begin{bmatrix}|&|&&|\\\\x^{(1)}&x^{(2)}&\\cdots&x^{(m)}\\\\|&|&&|\\end{bmatrix}, where \\,X \\in \\mathbb{R}^{n_x\\times\\, m}\\tag{2}$$\n",
        "\n",
        "We want to construct a vector $Z$, in which \n",
        "$$Z = \\begin{bmatrix}z^{(1)}z^{(2)}\\cdots z^{(m)}\\end{bmatrix}$$\n",
        "\n",
        "$$ =\\mathcal{w}^T X + \\begin{bmatrix}\\underbrace{b\\,b\\,b\\,b\\,b\\,b\\,b\\,b\\,b\\,b\\,b\\,b\\,b\\,b\\,b\\,b \\cdots b}\\\\1\\times\\,m\\end{bmatrix}\\\\=\\begin{bmatrix}\\mathcal{w}^Tx^{(1)} + b&\\mathcal{w}^Tx^{(2)} + b&\\cdots&\\mathcal{w}^Tx^{(m)} + b\\end{bmatrix}\\\\=\\mathcal{w}^TX + b \\tag{3}$$\n",
        "\n",
        "In python, we use $numpy.dot(\\mathcal{w}.T,X) + b$ to compute $Z$. The *Broadcasting* technique in python will broadcast the shape $(1,1)$ scalar to the same shape of the first item before adding to it.\n",
        "\n",
        "Then, we calculate $A = \\begin{bmatrix}a^{(1)}&a^{(2)}&\\cdots& a^{(m)}\\end{bmatrix}$ by calculate $\\sigma(Z)$:\n",
        "\n",
        "$$A = \\sigma(Z)\\tag{4}$$\n",
        "\n",
        "Therefore, the cost should be calculated by ('$\\cdot$' down below stands for normal matrices multiplication ):\n",
        "\n",
        "$$J = -\\cfrac{1}{m}[Y\\cdot\\log{A^T}+(1-Y)\\cdot\\log{((1-A)^T)}]\\tag{5}$$"
      ]
    },
    {
      "metadata": {
        "id": "BVehm8kQZ_Ho",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.24 Vectorizing Logistic Regression's Gradient Output##\n",
        "\n",
        "**Back Propgation**\n",
        "\n",
        "Recall we construct vector $\\mathrm{d}Z = \\begin{bmatrix}\\mathrm{d}z^{(1)}\\,\\mathrm{d}z^{(2)}\\,\\cdots\\,\\mathrm{d}z^{(m)}\\end{bmatrix}$ by calculating $\\mathrm{d}z^{(1)} = a^{(1)} - y^{(1)}, \\mathrm{d}z^{(2)} = a^{(2)} - y^{(2)}$ ...\n",
        "\n",
        "Since we have $A = \\begin{bmatrix}a{(1)}&a{(2)}&\\cdots&a{(m)}\\end{bmatrix}$,$Y = \\begin{bmatrix}y{(1)}&y{(2)}&\\cdots&y{(m)}\\end{bmatrix}$, \n",
        "\n",
        "$$\\mathrm{d}Z =A - Y\\\\= \\begin{bmatrix}a^{(1)} - y^{(1)},a^{(2)} - y^{(2)},\\cdots,a^{(m)} - y^{(m)}\\end{bmatrix}\\tag{1}$$\n",
        "\n",
        "Now we are going to get rid of the inner for-loop with the same technique:\n",
        "\n",
        "$$\\mathrm{d}b = \\cfrac{1}{m}\\sum_{i=1}^{m}\\mathrm{d}z^{(i)}\\\\= \\cfrac{1}{m}\\,numpy.sum(\\mathrm{d}Z)\\\\=\\cfrac{1}{m}\\,numpy.sum(A-Y)\\tag{2}$$\n",
        "\n",
        "$$\\mathrm{d}\\mathcal{w} = \\cfrac{1}{m}X\\mathrm{d}Z^T\\\\= \\cfrac{1}{m}\\begin{bmatrix}|&|&&|\\\\x^{(1)}&x^{(2)}&\\cdots&x^{(m)}\\\\|&|&&|\\end{bmatrix}\\begin{bmatrix}\\mathrm{d}z^{(1)}\\\\\\vdots\\\\\\mathrm{d}z^{(m)}\\end{bmatrix}\\\\= \\cfrac{1}{m}\\begin{bmatrix}x^{(1)}\\mathrm{d}z^{(1)}+\\cdots+ x^{(m)}\\mathrm{d}z^{(m)}\\end{bmatrix}\\\\=\\cfrac{1}{m}X(A-Y)^T\\tag{3}$$\n",
        "\n",
        "where $\\mathrm{d}b\\in\\mathbb{R},\\mathrm{d}\\mathcal{w}\\in\\mathbb{R}^n$.\n",
        "\n",
        "So the parameters could be updated by:\n",
        "\n",
        "$$\\mathcal{w} := \\mathcal{w} - \\alpha\\cdot\\mathrm{d}\\mathcal{w}$$\n",
        "\n",
        "$$b  := b - \\alpha\\cdot\\mathrm{d}b\\tag{4}$$"
      ]
    },
    {
      "metadata": {
        "id": "CRQmpUWvjTgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.25 Broadcasting in Python##\n",
        "\n",
        "**Broadcasting example**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7EHuVCYNj_Em",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "404b66b4-df37-4c9e-dc84-0e41f9e3b700"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[56.0,0.0,4.0,68.0],\n",
        "              [1.2,104.0,52.0,8.0],\n",
        "              [1.8,135.0,99.0,0.9]])\n",
        "\n",
        "print(A)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 56.    0.    4.   68. ]\n",
            " [  1.2 104.   52.    8. ]\n",
            " [  1.8 135.   99.    0.9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0yNkGYHQkWK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72a0ceac-054b-4a94-991d-2995947e653e"
      },
      "cell_type": "code",
      "source": [
        "# sum vertically by set axis to \"0\"\n",
        "cal = A.sum(axis=0)\n",
        "print(cal)\n",
        "\n",
        "# sum horizontally by set axis to \"1\"\n",
        "acal = A.sum(axis=1)\n",
        "print(acal)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 59.  239.  155.   76.9]\n",
            "[128.  165.2 236.7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CMvTMtXskyGo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3c7a004a-0535-4c6d-e5a0-2c4eb99b597d"
      },
      "cell_type": "code",
      "source": [
        "percentage  = 100*A/cal.reshape(1,4)\n",
        "print(percentage)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[94.91525424  0.          2.58064516 88.42652796]\n",
            " [ 2.03389831 43.51464435 33.5483871  10.40312094]\n",
            " [ 3.05084746 56.48535565 63.87096774  1.17035111]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KS93p9hloPAe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**General Principal**\n",
        "\n",
        "So now let's get into a concrete example to see what is the general rule of Broadcasting.\n",
        "\n",
        "Say we want this two matrix and vector to add up together:\n",
        "\n",
        "$$\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix} + \\begin{bmatrix}100&200&300\\end{bmatrix}\\\\= \\begin{bmatrix}101&202&303\\\\104&205&306\\end{bmatrix}$$\n",
        "\n",
        "What has brodcasting done here?\n",
        "\n",
        "Techniquelly, it can be regarded as a problem to add up a $(m,n)$ matrix and a $(1,n)$ row vector. First, the program copied the row vector $m$ times, thus the vector tuned out to be a $(m,n)$ matrix, just like the shape of the first matrix. Now they could be added up together.\n",
        "\n",
        "Another example here:\n",
        "\n",
        "$$\\begin{bmatrix}1&2&3\\\\4&5&6\\end{bmatrix} + \\begin{bmatrix}100\\\\200\\end{bmatrix}\\\\=\\begin{bmatrix}101& 102&103\\\\204&205&206\\end{bmatrix}$$\n",
        "\n",
        "We add a $(m,n)$ matrix and a $(m,1)$ vector by copy the column vector $n$ times and add two matrices together.\n",
        "\n",
        " In Matlab/ Octave, the bsxfun() function basically does the same thing. "
      ]
    },
    {
      "metadata": {
        "id": "KlzcQGm3w1C7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.26 A Note on Python/ Numpy Vectors##"
      ]
    },
    {
      "metadata": {
        "id": "iGmXhuqxxc4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0014552a-7c5c-471b-da3b-77f0f985236c"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.random.randn(5)\n",
        "\n",
        "print(a)\n",
        "print(a.shape)\n",
        "print(a.T)\n",
        "print(np.dot(a,a.T))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.28909035  3.20207242  1.57667516  0.40958471 -0.11709792]\n",
            "(5,)\n",
            "[ 0.28909035  3.20207242  1.57667516  0.40958471 -0.11709792]\n",
            "13.004217149732693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zkHt91hyxs6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "1528b861-13b6-4fa9-c77d-47d4d547daee"
      },
      "cell_type": "code",
      "source": [
        "a = np.random.randn(5,1)\n",
        "print(a)\n",
        "print(a.shape)\n",
        "print(a.T)\n",
        "#give you an outer product here\n",
        "print(np.dot(a,a.T))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.083628  ]\n",
            " [ 0.13744904]\n",
            " [ 2.29834784]\n",
            " [ 0.89662913]\n",
            " [-2.13837141]]\n",
            "(5, 1)\n",
            "[[ 0.083628    0.13744904  2.29834784  0.89662913 -2.13837141]]\n",
            "[[ 0.00699364  0.01149459  0.19220623  0.0749833  -0.17882772]\n",
            " [ 0.01149459  0.01889224  0.31590571  0.12324082 -0.29391711]\n",
            " [ 0.19220623  0.31590571  5.28240281  2.06076563 -4.91472131]\n",
            " [ 0.0749833   0.12324082  2.06076563  0.8039438  -1.9173261 ]\n",
            " [-0.17882772 -0.29391711 -4.91472131 -1.9173261   4.57263227]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G4fwxMCRy05f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below will produce a vector with shape $(5, )$,which is called a \"rank 1 \n",
        "```\n",
        "a = np.random.randn(5) \n",
        "```\n",
        "array\". A \"rank 1 array\" is neither a row vector nor a column vector, we should avoid using rank 1 arrays in practical.\n",
        "\n",
        "Instead , the code below shows a more proper conduct of using Python vectors.\n",
        "\n",
        "```\n",
        "a = np.random.randn(5,1)# a is a column vector\n",
        "a = np.random.randn(1,5)# a is a row vector\n",
        "```\n",
        "\n",
        "We should not be shy about using the economic $assert()$ to assure we are on the right track:\n",
        "\n",
        "```\n",
        "assert(a.shape == (5,1))\n",
        "```\n",
        "Also, calling $reshape()$ function is a good way to convert rank 1 arrays to a normal row/ column vector:\n",
        "\n",
        "```\n",
        "a = a.reshape((5,1))\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "j9uyhEgG4Juz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.28 Explanation of Logistic Regression Cost Function##\n",
        "\n",
        "**Cost on single example**\n",
        "\n",
        "$$If \\quad y = 1:\\quad p(y|x) = \\hat{y}\\\\If \\quad y = 0:\\quad p(y|x) = 1 - \\hat{y}\\tag{1}$$\n",
        "\n",
        "merge this two equations to a single equation:\n",
        "\n",
        "$$p(y|x) = \\hat{y}^y\\cdot(1-\\hat{y})^{(1 - y)}\\tag{2}$$\n",
        "\n",
        "$$\\log{p(y|x)} = \\log\\hat{y}^y(1 - \\hat{y})^{(1-y)}\\\\=y\\log{\\hat{y}} + (1-y)\\log{(1-\\hat{y})}\\\\= -\\mathcal{L}(\\hat{y},y)$$\n",
        "\n",
        "**Cost on m examples**\n",
        "\n",
        "$$\\max P(labels\\, in\\, training\\, set) \\\\\\Rightarrow\\max \\log{P(labels\\, in\\, training\\, set)}\\\\= \\max\\log{\\prod_{i=1}^{m}p(y^{(i)}|x^{(i)})}\\\\= \\max -\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})\\\\= min\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^{(i)},y^{(i)})\\tag{3}$$\n",
        "\n",
        "hence, our cost function (cost objective) should be like:\n",
        "\n",
        "$$\\min J(\\mathcal{w},b) = \\cfrac{1}{m}\\sum_{i=1}^{m}\\mathcal{L}(\\hat{y}^{(i)},y{(i)})\\tag{4}$$\n",
        "\n",
        "To summarize, by minimizing this cost function $J(\\mathcal{w},b)$ we're really carrying out maximum likelihood estimation with the logistic regression model. Under the assumption that our training examples were *IID*, or identically independently distributed."
      ]
    },
    {
      "metadata": {
        "id": "OVg9q1ZLZOBw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.30 Recap and Quiz##\n",
        "\n",
        "**Multiplications in Python**\n",
        "\n",
        "Here are some examples:"
      ]
    },
    {
      "metadata": {
        "id": "52K2h3LnXEcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "4df5f6d0-ec86-414b-a0a2-82c8e673ccf3"
      },
      "cell_type": "code",
      "source": [
        "#numpy.multipy() and '*' only used when matrix element-wise multiplcation\n",
        "a = np.random.randn(4, 3) # a.shape = (4, 3)\n",
        "b = np.random.randn(3, 2) # b.shape = (3, 2)\n",
        "c = a*b"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-20aba40ca292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# a.shape = (4, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# b.shape = (3, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,3) (3,2) "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "wuVvPsc1XKF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df682ae9-a693-40ce-dae5-42b7093d810c"
      },
      "cell_type": "code",
      "source": [
        "#numpy.dot() used to calculate noraml matrices multiplications\n",
        "a = np.random.randn(12288, 150) # a.shape = (12288, 150)\n",
        "b = np.random.randn(150, 45) # b.shape = (150, 45)\n",
        "c = np.dot(a,b)\n",
        "\n",
        "print(c.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12288, 45)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hz_3FEvmXWuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b942756a-329f-40c6-a291-78fd4e78987e"
      },
      "cell_type": "code",
      "source": [
        "#numpy.multipy() and '*' only used when matrix element-wise multiplcation\n",
        "a = np.random.randn(3, 3)\n",
        "b = np.random.randn(3, 1)\n",
        "c = a*b\n",
        "d = np.multiply(a,b)\n",
        "e = np.dot(a,b)\n",
        "print(c)\n",
        "print(d)\n",
        "print(e)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.19520437 -0.30376047  0.090505  ]\n",
            " [-0.0919571   0.03491028  0.30758659]\n",
            " [-0.40744506 -2.54631377 -1.97409886]]\n",
            "[[ 0.19520437 -0.30376047  0.090505  ]\n",
            " [-0.0919571   0.03491028  0.30758659]\n",
            " [-0.40744506 -2.54631377 -1.97409886]]\n",
            "[[-0.19931045]\n",
            " [-1.96073588]\n",
            " [-1.39225455]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w3NKbf_pX0-d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bd3856e8-489f-40fc-9539-c15533dc9e60"
      },
      "cell_type": "code",
      "source": [
        "#numpy.dot() used to calculate noraml matrices multiplications\n",
        "a = np.random.randn(5,1)\n",
        "b = np.random.randn(5,1)\n",
        "c = np.dot(a,b.T)\n",
        "d = np.dot(a.T,b)\n",
        "\n",
        "print(c)\n",
        "print(d)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.76186001 -1.19280924 -0.62954887 -0.22274173  1.14507213]\n",
            " [ 1.31865715 -2.06456097 -1.08964784 -0.38553011  1.9819357 ]\n",
            " [ 1.29139663 -2.02188042 -1.06712162 -0.37756007  1.94096326]\n",
            " [ 0.6724741  -1.05286183 -0.55568648 -0.19660836  1.01072552]\n",
            " [-0.0682342   0.10683116  0.05638407  0.01994934 -0.1025557 ]]\n",
            "[[-2.66898663]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YMKRy-6RbPb5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ca8569e4-97bf-4f7d-f249-6e2507aecfc0"
      },
      "cell_type": "code",
      "source": [
        "#'**' and numpy.square() are used to element-wise compute the squares \n",
        "a = np.random.randn(2,5)\n",
        "b = a**2\n",
        "c= np.square(a)\n",
        "\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.35095658  0.08055802 -2.26520695 -0.66165286  1.81847212]\n",
            " [-0.58203692  1.09517407  2.32967058  0.87431618 -0.47255583]]\n",
            "[[1.82508368 0.00648959 5.13116251 0.43778451 3.30684087]\n",
            " [0.33876698 1.19940623 5.42736501 0.76442879 0.22330901]]\n",
            "[[1.82508368 0.00648959 5.13116251 0.43778451 3.30684087]\n",
            " [0.33876698 1.19940623 5.42736501 0.76442879 0.22330901]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lLbSnF-os7RF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**numpy.where()**\n",
        "\n",
        "[see blog](https://www.cnblogs.com/massquantity/p/8908859.html)"
      ]
    },
    {
      "metadata": {
        "id": "n3Ol_ml4czAD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}